{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a logistic regression model to classify more than two classes. We will use a data set comprising of images. And build an image classifier.\n",
    "1. Use a one vs rest classifier\n",
    "2. Use a cross entropy loss classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir=\"E:\\Work\\Machine Learning Course\\Python\\Module 3 Logistic Regression\\Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using scikit-learn. In this demo, we will build an image classifier. The data set we will use is a very popular open source data set called mnist data set. This data set contains images of handwritten digits ranging from 0 to 9, so there are in total 10 classes. We will use these images to build a model, that will be able to classify an image into one of the ten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build a model, we first would need to understand how images are stored in a computer and how we can represent these images in a manner suitable to do machine learning. For this simple demo, we will confine our discussion to black and white images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing Image Data for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image1.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are represented in the form of a matrix of numbers inside the computer. The matrix contains the pixel intensities; pixel intensities range from 0 to 255, a pixel intensity of 0 signifies a black region, while a pixel intensity of 255 signifies a white region in the image. For example, this image can be represented as a matrix where each number in the matrix can lie between 0 to 255 depending upon if that number is representing a dark region in the image or a light region in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening an Image Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"flatten.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this schematic shows how a digit 1 might be represented as an image. This is how a digit 1 will be perceived by human eye, the numbers here signify the pixel positions, so 1 here represents the first pixel, 2 here represents the second pixel, 3 here represents the third pixel and so on. Now this is the matrix representation of this image, as you can see, these were the light regions and hence, the pixel intensities are 255, while for the pixels at position 2, 5, 8 and 11, we have a pixel intensity of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do machine learning, we usually have many images. Now how would a predictor matrix be constructed for all these images? The way we represent each image in our predictor matrix is by flattening the image. This is what flattening an image means. We create a vector out of a matrix. So each vector now contains the pixel intensities that were present in the image matrix. We stack all these images in this way in a single predictor matrix where each row then becomes the vector representation of each image. This is one of the most popular ways of deriving a predictor matrix from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# os.chdir(data_dir)\n",
    "pixel_values=pd.read_csv(\"mnist_x.csv\")\n",
    "image_labels=pd.read_csv(\"mnist_y.csv\",header=None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2     3     4     5    6    7    8    9  ...   54   55   56  \\\n",
       "0  0.0  0.0  5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0  ...  5.0  0.0  0.0   \n",
       "3  0.0  0.0  7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0  ...  9.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "    57   58    59    60    61   62   63  \n",
       "0  0.0  6.0  13.0  10.0   0.0  0.0  0.0  \n",
       "1  0.0  0.0  11.0  16.0  10.0  0.0  0.0  \n",
       "2  0.0  0.0   3.0  11.0  16.0  9.0  0.0  \n",
       "3  0.0  7.0  13.0  13.0   9.0  0.0  0.0  \n",
       "4  0.0  0.0   2.0  16.0   4.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the first file looks like. This first file contains the flattened pixel values for each of the images, so we have many images in our data set and each of the images has been fattened. The original images were 8x8 and when we flattened them, we ended up having a single vector of length 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  1\n",
       "2  2\n",
       "3  3\n",
       "4  4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this contains the actual labels of the images, for example the flattened image in the first row is an image of digit 0; the flattened image in the second rows set is an image of digit 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we will do is we will normalize the values in our predictor matrix. The way we will do that is by dividing each value in this matrix by 255. As you know the values can range between 0 to 255, so if we divide by 255, all the numbers in this matrix will be bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing the pixel values\n",
    "pixel_values=pixel_values/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pixel_values\n",
    "y=image_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "X_train,X_test,y_train,y_test=model_selection.train_test_split(X,y,test_size=0.20,random_state=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen logistic regression class earlier in the context of binary classification. If we want to solve a multiclass classification problem using logistic regression, then we can use this parameter called multiclass and give it different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as discussed previously, there are 2 ways to do multiclass classification, one way is to use a 1 versus all approach or 1 versus rest approach; the other is to use a multinomial classifier or a cross entropy classifier. Now the logistic regression method in scikit-learn supports both these approaches. If we give a value of OVR to this multiclass parameter, we will be building a multiclass classifier using 1 versus rest strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as discussed earlier, logistic regression can be regularized, here I will using an L2 penalty. So lets instantiate a classification object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.linear_model as linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(multi_class=\"ovr\",penalty=\"l2\",solver=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solver{â€˜newton-cgâ€™, â€˜lbfgsâ€™, â€˜liblinearâ€™, â€˜sagâ€™, â€˜sagaâ€™}, default=â€™lbfgsâ€™ -- Algorithm to use in the optimization problem. The choice of the algorithm depends on the penalty chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, I will need to specify the appropriate value for the parameter C which corresponds to the strength of the regularization penalty that I have applied. Now for me to figure out a good value of C, I need to do a grid search, this is what I am doing here, I am doing a simple grid search. I am searching over a grid of numbers ranging between 0.01 to 10 and there are 10 such numbers that I am doing a grid search on, lets run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(multi_class='ovr'),\n",
       "             param_grid={'C': array([9.47684625, 2.27320874, 5.94825724, 4.28880376, 7.64376546,\n",
       "       0.03857731, 3.58066258, 9.09785191, 4.56624904, 9.81820906])})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(200)\n",
    "mod=model_selection.GridSearchCV(clf,param_grid={\"C\":np.random.uniform(0.01,10,10)})\n",
    "mod.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 9.818209064491702}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## accuracy score\n",
    "mod.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets obtain probability predictions from this model for the first row in our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.00784314, 0.04705882, 0.05098039, 0.0627451 ,\n",
       "        0.05882353, 0.00392157, 0.        , 0.        , 0.03137255,\n",
       "        0.0627451 , 0.05490196, 0.04313725, 0.02745098, 0.        ,\n",
       "        0.        , 0.        , 0.03137255, 0.0627451 , 0.02745098,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02745098, 0.0627451 , 0.0627451 , 0.04313725, 0.00392157,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "        0.02352941, 0.05882353, 0.03529412, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.03529412,\n",
       "        0.05882353, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.02745098, 0.0627451 , 0.04313725, 0.        ,\n",
       "        0.        , 0.        , 0.00392157, 0.0627451 , 0.0627451 ,\n",
       "        0.05098039, 0.00392157, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0].values.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape(1, -1) if it contains a single sample. We could change our Series into a NumPy array and then reshape it to have two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01435224, 0.03089049, 0.01674947, 0.11443426, 0.01403317,\n",
       "        0.69181214, 0.00932715, 0.01196723, 0.05155476, 0.04487909]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.predict_proba(X_test.iloc[0].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so since there are 10 classes, this will give me probability corresponding to each class or for whichever class the probability is high, I will make the prediction saying that this row belongs to that particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out which index has the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(mod.predict_proba(X_test.iloc[0].values.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to plot the first row in our test data set and see what kind of image it contains. Now if you remember these images are already flattened, so I will need to reshape them and if you remember, my original data set was flattened where each image is represented by a vector of length 64, so if I assume that my image is a square image which has been flattened, then the original image will have a dimension of 8x8. So this is what I am doing here, I am taking the first row in my data set and reshaping it into an 8x8 matrix and plotting it using matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ec795143c8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.array(X_test.iloc[0]).reshape(8,8),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this seems to be an image of five, a handwritten digit of five and this is what I have predicted, a maximum value was for a class of 5. So my model seems to be doing a good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s see, how we can build a cross entropy loss classifier. The only thing that we will need to change is, we will need to change the value that we passed to the multiclass parameter, instead of OVR, we will pass it a value of multinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VK\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(multi_class='multinomial'),\n",
       "             param_grid={'C': array([9.47684625, 2.27320874, 5.94825724, 4.28880376, 7.64376546,\n",
       "       0.03857731, 3.58066258, 9.09785191, 4.56624904, 9.81820906])})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(200)\n",
    "clf=linear_model.LogisticRegression(multi_class=\"multinomial\",penalty=\"l2\",solver=\"lbfgs\")\n",
    "mod1=model_selection.GridSearchCV(clf,param_grid={\"C\":np.random.uniform(0.01,10,10)})\n",
    "mod1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 9.818209064491702}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9290360046457609"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## accuracy score\n",
    "mod1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9472222222222222"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## score on test data\n",
    "mod1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00773295, 0.01586381, 0.00925783, 0.05749544, 0.00751203,\n",
       "        0.83803796, 0.00549471, 0.00719461, 0.02686031, 0.02455036]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1.predict_proba(X_test.iloc[0].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(mod1.predict_proba(X_test.iloc[0].values.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ec7a5925c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(X_test.iloc[0]).reshape(8,8),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
